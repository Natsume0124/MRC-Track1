<p>OpenAI o1 and DeepSeek-R1 have opened up the era of large reasoning models (LRM). Both
the strong semantic intelligence of LLM and the long-chain reasoning ability of LRM have brought
new opportunities and challenges to CV community. Now it is the time to deeply connect the
communities of CV and LLM & LRM to discuss how to further develop CV research to
tackle more complex tasks that need complex reasoning, towards System2 and beyond.</p>
<p>Multimodal Reasoning is essential for a wide range of applications. Traditionally, reasoning models
were developed under a closed-set paradigm, assuming fixed data distributions, categorical labels, and
predefined output formats. However, these models often struggle in real-world environments, which
are dynamic, vast, and physical. To address this, modern LRMs have evolved to tackle open-world
tasks through a more flexible, instruction-driven framework. By training on large-scale, multi-source
datasets, these models aim to robust handle a diverse range of reasoning tasks efficiently
with instructions or prompts and understand the scene intrinsically and physically. This
workshop aims to push the multimodal reasoning capabilities of large models beyond traditional fixed
tasks and explore how models comprehend complex relationships, such as object interactions within a
complex scene, zero-shot generalization, through a slow-thinking manner (Neuro-Symbolic, Chainof-Thought, Multi-step Reasoning, etc.). By addressing these aspects, we aim to bridge the gap
between constrained, structured data and complex open-world reasoning, fostering more flexible and
robust understanding in AI. The goal of this workshop is to bring together perspectives from
multiple disciplines (e.g., computer vision, multimodal learning, large language models)
to highlight major open questions and to identify collaboration opportunities to address
outstanding challenges in the domain of multimodal reasoning and slow thinking in the
context of large reasoning models.</p>
<p>This year we establish three challenges on Multimodal Reasoning Competition:</p>
<h5>Visual Grounding in Real-world Scenarios (VG-RS)</h5>
<p>Evaluating the model's scene perception, object localization, and spatial reasoning abilities in complex multimodal scenarios.</p>
<p>
  Task 1's simple baseline has been open-sourced at
  <a href="https://github.com/Lens4MLLMs/MARS2_Track1" target="_blank" rel="noopener noreferrer">
    https://github.com/Lens4MLLMs/MARS2_Track1
  </a>
</p>
<h5>Dataset Overview</h5>
<p>The custom-made dataset has 2K+ images, 1.5K+ videos, 17K+ question-answer pairs, and
15K+ bbox annotations. It is in its final stages of development, with data organization and ethical
checks underway, and is expected to be released in April. As a contingency plan, we will offer alternative
benchmarking tasks using existing datasets and provide an initial version of the new dataset.</p>

<p>We have additionally opened the following permanent closed testing channels to provide more metrics for visual grounding tasks.</p>






